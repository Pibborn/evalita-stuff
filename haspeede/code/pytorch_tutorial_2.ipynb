{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch-tutorial-2.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "N5imyfmRfRPP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The ```torch.nn``` package\n",
        "\n",
        "The ```torch.nn``` package provides a number of higher-level APIs that resemble tensorflow's ```layers``` or Keras. Building a simple MLP is therefore as easy as it should be:"
      ]
    },
    {
      "metadata": {
        "id": "eCyurDmOjIOT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oxU7DWTAkajT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model definition"
      ]
    },
    {
      "metadata": {
        "id": "6d139zcqeEZI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "num_features = 28*28\n",
        "num_classes = 10\n",
        "H_size = 100\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "  torch.nn.Linear(num_features, H_size),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Linear(H_size, num_classes)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OW8nHC5rkW-U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset loading"
      ]
    },
    {
      "metadata": {
        "id": "GYHTEs2UjEnb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])),\n",
        "        batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])),\n",
        "        batch_size=1, shuffle=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F5FGDH6rkd8W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ]
    },
    {
      "metadata": {
        "id": "aLk5BVEAj-vL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "learning_rate = 1e-5\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  # train \n",
        "  for j, (x, y_true) in enumerate(train_loader):\n",
        "    x = x.view(batch_size, num_features)\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(x)\n",
        "    loss_value = loss(y_pred, y_true)\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "    if j % 100 == 0:\n",
        "      print('Epoch {}; batch {}: loss {}'.format(i, j, loss_value.detach().numpy()))\n",
        "  # test\n",
        "  loss_value_test = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for x, y_true in test_loader:\n",
        "      x = x.view(1, num_features)\n",
        "      y_pred = model(x)\n",
        "      loss_value_test += loss(y_pred, y_true)\n",
        "      y_pred = y_pred.max(1, keepdim=True)[1]\n",
        "      correct += y_pred.eq(y_true).sum().numpy()\n",
        "  loss_value_test /= len(test_loader.dataset)\n",
        "  accuracy = correct / len(test_loader.dataset)\n",
        "  print('Epoch {}: loss {} accuracy {}'.format(i, loss_value_test, accuracy))\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A-3yP-5jynVm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extending the nn.Module class\n",
        "\n",
        "Another cool thing about PyTorch is that it kind of plays nice with the object-oriented model. When you build a neural net, you can extend the base ```nn.Module``` class. This might be overkill in the case of a MLP or a very simple feedforward model in general, since the ```nn.Sequential```\n",
        "should have you set; however, it gives you some incentive to write clean code and a natural way to handle hyperparameters (constructor parameters\n",
        "in your new class)."
      ]
    },
    {
      "metadata": {
        "id": "Ablw6TQozRAh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class FCNet(nn.Module):\n",
        "  def __init__(self, H1, H2):\n",
        "    super(FCNet, self).__init__()\n",
        "    self.fc1 = nn.Linear(num_features, H1)\n",
        "    self.fc2 = nn.Linear(H1, H2)\n",
        "    self.fc3 = nn.Linear(H2, num_classes)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    y_pred = self.fc3(x)\n",
        "    return y_pred\n",
        "  \n",
        "  \n",
        "model = FCNet(100, 50)\n",
        "\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "learning_rate = 1e-5\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "num_epochs = 10\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  # train \n",
        "  model.train() #!!!\n",
        "  for j, (x, y_true) in enumerate(train_loader):\n",
        "    x = x.view(batch_size, num_features)\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(x)\n",
        "    loss_value = loss(y_pred, y_true)\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "    if j % 100 == 0:\n",
        "      print('Epoch {}; batch {}: loss {}'.format(i, j, loss_value.detach().numpy()))\n",
        "  # test\n",
        "  loss_value_test = 0\n",
        "  correct = 0\n",
        "  model.eval() #!!!\n",
        "  for x, y_true in test_loader:\n",
        "    x = x.view(1, num_features)\n",
        "    y_pred = model(x)\n",
        "    loss_value_test += loss(y_pred, y_true)\n",
        "    y_pred = y_pred.max(1, keepdim=True)[1]\n",
        "    correct += y_pred.eq(y_true).sum().numpy()\n",
        "  loss_value_test /= len(test_loader.dataset)\n",
        "  accuracy = correct / len(test_loader.dataset)\n",
        "  print('Epoch {}: loss {} accuracy {}'.format(i, loss_value_test, accuracy))\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4g6wsenq4GcW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Recurrent Networks with PyTorch\n",
        "\n",
        "Thanks to its dynamic graph building, PyTorch is a good fit for training Recurrent networks. While recurrent networks can be thought as a feedforward model that has loops, like so\n",
        "\n",
        "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png\" height=\"300px;\"/>\n",
        "\n",
        "another way to think about it, perhaps more explicit, comes from **unrolling the loop**: \n",
        "\n",
        "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" height=\"300px;\"/>\n",
        "\n",
        "*(images from [Chris Olah's blog](http://colah.github.io))*\n",
        "\n",
        "Here, $X_0, X_1, ...X_t$ are elements of the sequence we wish to process. Since there is no fixed length for our sequence, a library that builds a static computational graphs would resort to some kind of trick here. From what I have gathered and experienced personally, TensorFlow requires you to specify the length of each of your input sequences and to pad them with zeros until they are as long as the longest one. PyTorch, on the other hand, will build the computational graph with the correct number of nodes/tensors/operations depending on the length of the sequence you are passing to the model as input.\n",
        "\n",
        "\n",
        "The specific recurrent network flavor we will try today is a *many-to-one* model: we want to read a whole sequence (actually a sentence) and return a class label. This is called a many-to-one model because we only want to predict a single value while having multiple values as an input; alternatives would be many-to-many (machine translation), one-to-many (image captioning) and one-to-one (traditional feedforward networks). \n",
        "\n",
        "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" height=\"300px;\"/>\n",
        "\n",
        "*(image from [Andrej Karpathy's blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/))*\n",
        "\n",
        "To get an intuition for what a recurrent network does, reason about its three separate set of parameters:\n",
        "\n",
        "* **Input-hidden parameters** which map an element of the sequence to the hidden layer neurons (red lines in the figure above);\n",
        "* **hidden-hidden parameters** which map the *evolution* of your sequence and map the network's hidden state at step $t-1$ to step $t$ (green lines);\n",
        "* **hidden-output parameters** which map the network's hidden state to the output layer neurons, which you may softmax to output a prediction (blue lines).\n",
        "\n",
        "Let us call these separate set of parameters $V_{ih}$, $V_{hh}$ and $V_{ho}$. Then we can write the core equation for RNNs, which computes the activation of the hidden layer, as the following:\n",
        "\n",
        "$$\n",
        "h_t = tanh(V_{ih} x_t + b_{ih} + V_{hh}h_{t-1} + b_{hh})\n",
        "$$\n",
        "\n",
        "In our case, let's name $T$ the last timestep for a given sequence. To get a prediction for our sequence, we will compute the following:\n",
        "\n",
        "$$\n",
        "y = argmax(softmax(V_{ho}h_T + b_{ho}))\n",
        "$$\n",
        "\n",
        "These equation may be implemented directly, without the need to use a pre-made module. Obviously, this kind of \"recurrent layer\" is available in PyTorch, alongside many others (```RNN```, ```LSTM```, ```GRU```...); however, here we will just use the fully-connected ```Linear``` layer we introduced just above.\n",
        "\n",
        "One thing we also have to decide is which kind of vectors we will have in our sequence $X_0 ... X_T$. There are two possible choices here: using **characters** or **words**. Imagining having to model a distribution $P(y \\mid \\textbf{x})$ where \\textbf{x} is either the distribution of words or characters over our dataset, its dimensionality will be higher when using words, as there are many more possible words than characters; however, it seems hard to figure out the meaning of a sentence using its characters. On top of that, if we build a word-level RNN we might leverage pre-computed **word embeddings** [1]. For this reason, a Word-level RNN is the choice that makes most sense to me in this setting.\n",
        "\n",
        "Just below, I defined two RNNs, ```CharRNN``` and ```MyRNN```,  which work on characters and words respectively. The representation I chose to model characters is just simple one-hot encoding; the Italian word embeddings are provided by the Polyglot library, and I have been told that they have been extracted from the Italian Wikipedia corpus. \n",
        "\n",
        "\n",
        "\n",
        "[[1]](https://arxiv.org/abs/1301.3781): Mikolov et al., Efficient Estimation of Word Representations in Vector Space \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8XMPApmc2A30",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class CharRNN(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(CharRNN, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    \n",
        "    self.Vih = nn.Linear(input_size, hidden_size)\n",
        "    self.Vhh = nn.Linear(hidden_size, hidden_size)\n",
        "    self.Vho = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    hidden_state = torch.zeros(self.hidden_size)\n",
        "    for xi in x:\n",
        "      xi = torch.Tensor(onehot_dict[xi.lower()])\n",
        "      hidden_state = F.relu(self.Vih(xi) + self.Vhh(hidden_state))\n",
        "    return self.Vho(hidden_state)\n",
        "  \n",
        "  \n",
        "class MyRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(MyRNN, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    \n",
        "    self.Vih = torch.nn.Parameter(data=torch.randn(input_size, hidden_size))\n",
        "    self.bih = torch.nn.Parameter(data=torch.randn(hidden_size))\n",
        "    self.Vhh = torch.nn.Parameter(data=torch.randn(hidden_size, hidden_size))\n",
        "    self.bhh = torch.nn.Parameter(data=torch.randn(hidden_size))\n",
        "    self.Vho = torch.nn.Parameter(data=torch.randn(hidden_size))\n",
        "    self.bho = torch.nn.Parameter(data=torch.randn(output_size))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    hidden_state = torch.zeros(self.hidden_size)\n",
        "    for xi in x:\n",
        "      xi = torch.Tensor(xi)\n",
        "      hidden_state = F.tanh((torch.matmul(xi, self.Vih) + self.bih) + (torch.matmul(hidden_state, self.Vhh) + self.bhh)) \n",
        "    return torch.matmul(hidden_state, self.Vho) + self.bho\n",
        "   \n",
        "\n",
        "model = MyRNN(5, 10, 2)\n",
        "x = np.array([[0, 1, 0, 1, 1], [1, 0, 1, 1, 1], [1, 1, 0, 1, 1]])\n",
        "y_ = model(torch.Tensor(x))\n",
        "print(y_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0rbjI-NGcqoh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Haspeede\n",
        "\n",
        "Haspeede is the [hate speech detection task](http://www.di.unito.it/~tutreeb/haspeede-evalita18/index.html) @ EvalIta 2018. It is a binary classification task about deciding whether a social media comments constitutes an act of hate speech. While definitions of this phenomenon are evasive, some examples can give us at least some kind of intuition about it:\n",
        "\n",
        "* **Encouraging or justifying violence**: any kind of comment that depicts acts of violence against minorities, or encourages others to perform such acts can be described as *hate speech*;\n",
        "* **Encouraging hate and discrimination**: comments that actively promote discrimination or express hate towards minorities because of their intrisic characteristics\n"
      ]
    },
    {
      "metadata": {
        "id": "Sh9h2bB0igZ5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Haspeede data"
      ]
    },
    {
      "metadata": {
        "id": "ulmykxJPlz19",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -q gspread pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbq8lGBQlz3r",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "import numpy as np\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "worksheet = gc.open_by_url('https://docs.google.com/spreadsheets/d/1M5oznzGZgL24DtsVDnXbnnYIiKv9XGZIjE6AZmr-Nj4/').sheet1\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "\n",
        "data = np.array(rows)\n",
        "X = data[:, 0]\n",
        "y = data[:, 1].astype('int32')\n",
        "\n",
        "print(X[0])\n",
        "print(y[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ixjMSnn8o6Rf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QBeQsPsKwDZX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CharRNN - Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "V5LJ8MDapt1n",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def unique_char_dict(X):\n",
        "  d = {}\n",
        "  for x in X:\n",
        "    for xi in x:\n",
        "      try:\n",
        "        d[xi.lower()] += 1\n",
        "      except KeyError:\n",
        "        d[xi.lower()] = 1\n",
        "  return d\n",
        "\n",
        "def get_dropped_chars(d, thresh=100):\n",
        "  l = []\n",
        "  for char, occ in d.items():\n",
        "    if occ < 100:\n",
        "      l.append(char)\n",
        "  return l\n",
        "\n",
        "def drop_chars(X):\n",
        "  d = unique_char_dict(X)\n",
        "  dropped_chars_list = get_dropped_chars(d)\n",
        "  X_new = []\n",
        "  for x in X:\n",
        "    X_new.append([xi for xi in x if xi not in dropped_chars_list])\n",
        "  return X_new\n",
        "\n",
        "def get_onehot_dict(X, d):\n",
        "  temp_d = {}\n",
        "  for x in X:\n",
        "    for xi in x:\n",
        "      temp_d[xi.lower()] = 0\n",
        "    if len(temp_d) == len(d):\n",
        "      break\n",
        "  onehot_d = {}\n",
        "  for i, char in enumerate(temp_d.keys()):\n",
        "    onehot_d[char] = np.array([0 for el in temp_d.values()])\n",
        "    onehot_d[char][i] = 1\n",
        "  return onehot_d\n",
        "\n",
        "label_dict = {0: np.array([0, 1]), 1: np.array([1, 0])}\n",
        "\n",
        "X_new = drop_chars(X)\n",
        "char_dict = unique_char_dict(X_new)\n",
        "onehot_dict = get_onehot_dict(X_new, char_dict)\n",
        "#print(onehot_dict)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.3)\n",
        "print(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RxN73XIARLA8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CharRNN - training"
      ]
    },
    {
      "metadata": {
        "id": "fAY1SgDFu2uy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "num_features = len(onehot_dict)\n",
        "hidden_size = 100\n",
        "num_classes = 2\n",
        "\n",
        "model = CharRNN(num_features, hidden_size, num_classes)\n",
        "\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "learning_rate = 1e-5\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  # train\n",
        "  for j, (x, y_true) in enumerate(zip(X_train, y_train)):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(x)\n",
        "    loss_value = loss(y_pred.view(1, -1), torch.LongTensor([y_true]))\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "    if j % 100 == 0:\n",
        "      print('Epoch {}; batch {}: loss {}'.format(i, j, loss_value.detach().numpy()))\n",
        "  # test\n",
        "  loss_value_test = 0\n",
        "  correct = 0\n",
        "  for j, (x, y_true) in enumerate(zip(X_test, y_test)):\n",
        "    y_true = torch.LongTensor([y_true])\n",
        "    y_pred = model(x)\n",
        "    y_pred = y_pred.view(1, -1)\n",
        "    loss_value_test += loss(y_pred.view(1, -1), y_true)\n",
        "    y_pred = y_pred.max(1, keepdim=True)[1]\n",
        "    correct += y_pred.eq(y_true).sum().numpy()\n",
        "  loss_value_test /= (j+1)\n",
        "  accuracy = correct / (j+1)\n",
        "  print('Epoch {}: loss {} accuracy {}'.format(i, loss_value_test, accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DXfqGgyEUwpz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word-level RNN: Getting embeddings with Polyglot"
      ]
    },
    {
      "metadata": {
        "id": "b8UmHRFAz99U",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install polyglot pyicu pycld2 morfessor\n",
        "!polyglot download sgns2.it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4y5i01O0MjA9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Test: properties of summation in word embeddings (man - woman - king example). The expected result should be that $$roma - italia + germania \\sim berlino$$ \n",
        "\n",
        "However, you see that a random other word such as Washington is actually closer...\n",
        "\n",
        "Also observe that the original example ($king - man + woman \\sim queen$) actually kind of holds for the Italian embeddings, too!"
      ]
    },
    {
      "metadata": {
        "id": "mKICv7yoU4vi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "096b57d0-524d-4d58-ca2d-0a19f8d110ca",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530707687358,
          "user_tz": -120,
          "elapsed": 461,
          "user": {
            "displayName": "mattia cerrato",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115421962299646362504"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from polyglot.mapping import Embedding\n",
        "from polyglot.text import Word\n",
        "\n",
        "w1 = Word(\"re\", language=\"it\")\n",
        "w2 = Word(\"uomo\", language=\"it\")\n",
        "w3 = Word(\"donna\", language=\"it\")\n",
        "\n",
        "\n",
        "w4 = Word(\"gengive\", language=\"it\")\n",
        "w5 = Word(\"parroco\", language=\"it\")\n",
        "w6 = Word(\"principe\", language=\"it\")\n",
        "w7 = Word(\"regina\", language=\"it\")\n",
        "\n",
        "\n",
        "\n",
        "vect = w1.vector - w2.vector + w3.vector\n",
        "\n",
        "print(sum(w4.vector - vect)**2 / len(vect))\n",
        "print(sum(w5.vector - vect)**2 / len(vect))\n",
        "print(sum(w6.vector - vect)**2 / len(vect))\n",
        "print(sum(w7.vector - vect)**2 / len(vect))\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.007138776223034396\n",
            "0.2632947066149454\n",
            "0.3559078571242888\n",
            "0.024611842383821074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0rrqmAfZRo14",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word-level RNN: training"
      ]
    },
    {
      "metadata": {
        "id": "wIupBSnlVD8D",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from polyglot.text import Word\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def sentence2matrix(tokens):\n",
        "  x = []\n",
        "  for token in tokens:\n",
        "    try: \n",
        "      w = Word(token, language=\"it\").vector\n",
        "      x.append(w)\n",
        "    except KeyError:\n",
        "      pass\n",
        "  return np.array(x)\n",
        "\n",
        "def transform_embeddings(X):\n",
        "  X_new = []\n",
        "  for x in X:\n",
        "    tokens = nltk.word_tokenize(x)\n",
        "    x_new = sentence2matrix(tokens)\n",
        "    X_new.append(x_new)\n",
        "  return np.array(X_new)\n",
        "    \n",
        "X_w2v = transform_embeddings(X)\n",
        "\n",
        "print(X.shape)\n",
        "print(X_w2v.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bOYyEQGAMudk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Word-level RNN: Training loop "
      ]
    },
    {
      "metadata": {
        "id": "ea_nD54fmGzg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch.optim as optim\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_w2v, y, test_size=0.3)\n",
        "\n",
        "num_features = X_train[0].shape[1]\n",
        "num_hidden = 100\n",
        "num_classes = 2\n",
        "\n",
        "model = MyRNN(num_features, num_hidden, num_classes)\n",
        "\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "learning_rate = 1e-4\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  # train \n",
        "  model.train()\n",
        "  for j, (x, y_true) in enumerate(zip(X_train, y_train)):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(x)\n",
        "    y_pred = y_pred.view(1, -1)\n",
        "    loss_value = loss(y_pred, torch.LongTensor([y_true]))\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "    if j % 500 == 0:\n",
        "      print('Epoch {}; batch {}: loss {}'.format(i, j, loss_value.detach().numpy()))\n",
        "  # test\n",
        "  loss_value_test = 0\n",
        "  correct = 0\n",
        "  model.eval()\n",
        "  for j, (x, y_true) in enumerate(zip(X_test, y_test)):\n",
        "    y_true = torch.LongTensor([y_true])\n",
        "    y_pred = model(x)\n",
        "    y_pred = y_pred.view(1, -1)\n",
        "    loss_value_test += loss(y_pred, y_true)\n",
        "    y_pred = y_pred.max(1, keepdim=True)[1]\n",
        "    correct += y_pred.eq(y_true).sum().numpy()\n",
        "  loss_value_test /= (j+1)\n",
        "  accuracy = correct / (j+1)\n",
        "  print('Epoch {}: loss {} accuracy {}'.format(i, loss_value_test, accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YWsn4wFRqBqK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}